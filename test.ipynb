{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "92b177ae525d99aefec57d1e05087defe35c515de2a950e481ffeaeb3a04b84b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer,GPT2Tokenizer\n",
    "from transformers import BertForTokenClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./data.csv',converters={'header_anno':eval,'msg_anno':eval},index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2=GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 19082,  1362,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [  101, 19082,  1112,  1181,  8057,  1116,  1181,  8057,  1181,  2087,\n",
       "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "t1(['hello world','hello asdfasdfadf'],return_tensors='pt',padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=t1.tokenize('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[19082, 1362]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "t1.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [101, 19082, 1362, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "t1('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model=BertForTokenClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([1] * 256).unsqueeze(0) # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=torch.tensor([10]*256).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=tensor(1.0176, grad_fn=<NllLossBackward>), logits=tensor([[[ 0.2508, -0.3077],\n",
       "         [ 0.2337, -0.2084],\n",
       "         [ 0.2399, -0.2031],\n",
       "         [ 0.2407, -0.1955],\n",
       "         [ 0.2360, -0.1914],\n",
       "         [ 0.2469, -0.1748],\n",
       "         [ 0.2519, -0.1631],\n",
       "         [ 0.2557, -0.1600],\n",
       "         [ 0.2597, -0.1598],\n",
       "         [ 0.2597, -0.1663],\n",
       "         [ 0.2589, -0.1724],\n",
       "         [ 0.2609, -0.1747],\n",
       "         [ 0.2514, -0.1885],\n",
       "         [ 0.2458, -0.1881],\n",
       "         [ 0.2415, -0.1849],\n",
       "         [ 0.2362, -0.1890],\n",
       "         [ 0.2302, -0.1761],\n",
       "         [ 0.2360, -0.1700],\n",
       "         [ 0.2372, -0.1640],\n",
       "         [ 0.2479, -0.1530],\n",
       "         [ 0.2661, -0.1461],\n",
       "         [ 0.2664, -0.1447],\n",
       "         [ 0.2697, -0.1396],\n",
       "         [ 0.2810, -0.1462],\n",
       "         [ 0.2831, -0.1475],\n",
       "         [ 0.2827, -0.1474],\n",
       "         [ 0.2827, -0.1549],\n",
       "         [ 0.2795, -0.1513],\n",
       "         [ 0.2731, -0.1571],\n",
       "         [ 0.2648, -0.1555],\n",
       "         [ 0.2665, -0.1467],\n",
       "         [ 0.2653, -0.1371],\n",
       "         [ 0.2611, -0.1255],\n",
       "         [ 0.2694, -0.1198],\n",
       "         [ 0.2709, -0.1259],\n",
       "         [ 0.2715, -0.1218],\n",
       "         [ 0.2747, -0.1274],\n",
       "         [ 0.2695, -0.1361],\n",
       "         [ 0.2698, -0.1415],\n",
       "         [ 0.2671, -0.1521],\n",
       "         [ 0.2590, -0.1593],\n",
       "         [ 0.2616, -0.1580],\n",
       "         [ 0.2468, -0.1552],\n",
       "         [ 0.2370, -0.1520],\n",
       "         [ 0.2379, -0.1514],\n",
       "         [ 0.2417, -0.1497],\n",
       "         [ 0.2500, -0.1416],\n",
       "         [ 0.2595, -0.1349],\n",
       "         [ 0.2622, -0.1289],\n",
       "         [ 0.2661, -0.1266],\n",
       "         [ 0.2768, -0.1377],\n",
       "         [ 0.2793, -0.1322],\n",
       "         [ 0.2837, -0.1394],\n",
       "         [ 0.2812, -0.1472],\n",
       "         [ 0.2768, -0.1488],\n",
       "         [ 0.2782, -0.1519],\n",
       "         [ 0.2688, -0.1509],\n",
       "         [ 0.2595, -0.1398],\n",
       "         [ 0.2597, -0.1459],\n",
       "         [ 0.2585, -0.1462],\n",
       "         [ 0.2619, -0.1392],\n",
       "         [ 0.2717, -0.1422],\n",
       "         [ 0.2700, -0.1417],\n",
       "         [ 0.2732, -0.1493],\n",
       "         [ 0.2694, -0.1589],\n",
       "         [ 0.2748, -0.1651],\n",
       "         [ 0.2728, -0.1723],\n",
       "         [ 0.2627, -0.1909],\n",
       "         [ 0.2570, -0.2056],\n",
       "         [ 0.2416, -0.2141],\n",
       "         [ 0.2361, -0.2185],\n",
       "         [ 0.2343, -0.2161],\n",
       "         [ 0.2259, -0.2185],\n",
       "         [ 0.2317, -0.2116],\n",
       "         [ 0.2368, -0.2066],\n",
       "         [ 0.2447, -0.2128],\n",
       "         [ 0.2532, -0.2100],\n",
       "         [ 0.2559, -0.2241],\n",
       "         [ 0.2570, -0.2417],\n",
       "         [ 0.2588, -0.2572],\n",
       "         [ 0.2676, -0.2689],\n",
       "         [ 0.2622, -0.2818],\n",
       "         [ 0.2574, -0.2976],\n",
       "         [ 0.2601, -0.3156],\n",
       "         [ 0.2610, -0.3198],\n",
       "         [ 0.2544, -0.3230],\n",
       "         [ 0.2501, -0.3240],\n",
       "         [ 0.2558, -0.3131],\n",
       "         [ 0.2540, -0.3142],\n",
       "         [ 0.2619, -0.3124],\n",
       "         [ 0.2724, -0.3070],\n",
       "         [ 0.2757, -0.3179],\n",
       "         [ 0.2834, -0.3383],\n",
       "         [ 0.2781, -0.3489],\n",
       "         [ 0.2771, -0.3597],\n",
       "         [ 0.2707, -0.3734],\n",
       "         [ 0.2647, -0.3838],\n",
       "         [ 0.2555, -0.3998],\n",
       "         [ 0.2519, -0.3967],\n",
       "         [ 0.2433, -0.3972],\n",
       "         [ 0.2371, -0.3968],\n",
       "         [ 0.2276, -0.3996],\n",
       "         [ 0.2328, -0.3945],\n",
       "         [ 0.2403, -0.4137],\n",
       "         [ 0.2486, -0.4101],\n",
       "         [ 0.2552, -0.4243],\n",
       "         [ 0.2582, -0.4301],\n",
       "         [ 0.2575, -0.4397],\n",
       "         [ 0.2662, -0.4511],\n",
       "         [ 0.2637, -0.4639],\n",
       "         [ 0.2621, -0.4605],\n",
       "         [ 0.2640, -0.4727],\n",
       "         [ 0.2701, -0.4768],\n",
       "         [ 0.2660, -0.4761],\n",
       "         [ 0.2729, -0.4706],\n",
       "         [ 0.2699, -0.4638],\n",
       "         [ 0.2796, -0.4558],\n",
       "         [ 0.2885, -0.4557],\n",
       "         [ 0.2884, -0.4578],\n",
       "         [ 0.2976, -0.4620],\n",
       "         [ 0.3003, -0.4633],\n",
       "         [ 0.2918, -0.4598],\n",
       "         [ 0.2938, -0.4693],\n",
       "         [ 0.2832, -0.4800],\n",
       "         [ 0.2700, -0.4903],\n",
       "         [ 0.2670, -0.4944],\n",
       "         [ 0.2602, -0.4889],\n",
       "         [ 0.2442, -0.4787],\n",
       "         [ 0.2411, -0.4870],\n",
       "         [ 0.2413, -0.4891],\n",
       "         [ 0.2443, -0.4975],\n",
       "         [ 0.2485, -0.4799],\n",
       "         [ 0.2562, -0.4811],\n",
       "         [ 0.2667, -0.4831],\n",
       "         [ 0.2732, -0.4874],\n",
       "         [ 0.2831, -0.4845],\n",
       "         [ 0.2933, -0.4885],\n",
       "         [ 0.3007, -0.4848],\n",
       "         [ 0.3028, -0.4927],\n",
       "         [ 0.3049, -0.4849],\n",
       "         [ 0.3043, -0.5010],\n",
       "         [ 0.2966, -0.4909],\n",
       "         [ 0.2959, -0.5090],\n",
       "         [ 0.3020, -0.5115],\n",
       "         [ 0.2939, -0.4972],\n",
       "         [ 0.2802, -0.4888],\n",
       "         [ 0.2864, -0.4794],\n",
       "         [ 0.2856, -0.4714],\n",
       "         [ 0.2802, -0.4491],\n",
       "         [ 0.2734, -0.4480],\n",
       "         [ 0.2868, -0.4327],\n",
       "         [ 0.2872, -0.4205],\n",
       "         [ 0.2928, -0.4018],\n",
       "         [ 0.2936, -0.4045],\n",
       "         [ 0.2968, -0.4222],\n",
       "         [ 0.2862, -0.4259],\n",
       "         [ 0.2958, -0.4335],\n",
       "         [ 0.2867, -0.4294],\n",
       "         [ 0.2705, -0.4467],\n",
       "         [ 0.2596, -0.4546],\n",
       "         [ 0.2418, -0.4629],\n",
       "         [ 0.2536, -0.4573],\n",
       "         [ 0.2537, -0.4432],\n",
       "         [ 0.2428, -0.4442],\n",
       "         [ 0.2637, -0.4453],\n",
       "         [ 0.2610, -0.4424],\n",
       "         [ 0.2639, -0.4480],\n",
       "         [ 0.2654, -0.4436],\n",
       "         [ 0.2644, -0.4388],\n",
       "         [ 0.2698, -0.4527],\n",
       "         [ 0.2575, -0.4490],\n",
       "         [ 0.2508, -0.4519],\n",
       "         [ 0.2436, -0.4462],\n",
       "         [ 0.2504, -0.4478],\n",
       "         [ 0.2380, -0.4526],\n",
       "         [ 0.2511, -0.4318],\n",
       "         [ 0.2406, -0.4208],\n",
       "         [ 0.2609, -0.4076],\n",
       "         [ 0.2665, -0.3816],\n",
       "         [ 0.2905, -0.3731],\n",
       "         [ 0.2963, -0.3595],\n",
       "         [ 0.2971, -0.3610],\n",
       "         [ 0.2890, -0.3594],\n",
       "         [ 0.2982, -0.3745],\n",
       "         [ 0.3007, -0.3732],\n",
       "         [ 0.3009, -0.3708],\n",
       "         [ 0.2948, -0.3686],\n",
       "         [ 0.2803, -0.3808],\n",
       "         [ 0.2978, -0.3887],\n",
       "         [ 0.2741, -0.3915],\n",
       "         [ 0.2642, -0.3762],\n",
       "         [ 0.2537, -0.3827],\n",
       "         [ 0.2513, -0.3698],\n",
       "         [ 0.2361, -0.3802],\n",
       "         [ 0.2356, -0.3704],\n",
       "         [ 0.2187, -0.3600],\n",
       "         [ 0.2158, -0.3476],\n",
       "         [ 0.2126, -0.3455],\n",
       "         [ 0.2275, -0.3340],\n",
       "         [ 0.2418, -0.3162],\n",
       "         [ 0.2415, -0.3251],\n",
       "         [ 0.2526, -0.3162],\n",
       "         [ 0.2630, -0.3040],\n",
       "         [ 0.2774, -0.2917],\n",
       "         [ 0.2788, -0.3078],\n",
       "         [ 0.2854, -0.3082],\n",
       "         [ 0.2797, -0.2849],\n",
       "         [ 0.2820, -0.2984],\n",
       "         [ 0.2815, -0.3002],\n",
       "         [ 0.2689, -0.2930],\n",
       "         [ 0.2822, -0.3015],\n",
       "         [ 0.2676, -0.2889],\n",
       "         [ 0.2520, -0.2865],\n",
       "         [ 0.2558, -0.2717],\n",
       "         [ 0.2511, -0.2577],\n",
       "         [ 0.2524, -0.2645],\n",
       "         [ 0.2580, -0.2398],\n",
       "         [ 0.2659, -0.2564],\n",
       "         [ 0.2713, -0.2507],\n",
       "         [ 0.2745, -0.2568],\n",
       "         [ 0.2748, -0.2572],\n",
       "         [ 0.2651, -0.2626],\n",
       "         [ 0.2482, -0.2700],\n",
       "         [ 0.2447, -0.2709],\n",
       "         [ 0.2414, -0.2627],\n",
       "         [ 0.2364, -0.2470],\n",
       "         [ 0.2345, -0.2382],\n",
       "         [ 0.2327, -0.2412],\n",
       "         [ 0.2357, -0.2284],\n",
       "         [ 0.2475, -0.2224],\n",
       "         [ 0.2401, -0.2079],\n",
       "         [ 0.2631, -0.2002],\n",
       "         [ 0.2698, -0.1999],\n",
       "         [ 0.2693, -0.2052],\n",
       "         [ 0.2744, -0.1980],\n",
       "         [ 0.2745, -0.2009],\n",
       "         [ 0.2784, -0.2128],\n",
       "         [ 0.2795, -0.2085],\n",
       "         [ 0.2659, -0.2055],\n",
       "         [ 0.2722, -0.2297],\n",
       "         [ 0.2684, -0.2112],\n",
       "         [ 0.2578, -0.2028],\n",
       "         [ 0.2581, -0.2049],\n",
       "         [ 0.2633, -0.2018],\n",
       "         [ 0.2556, -0.2011],\n",
       "         [ 0.2601, -0.1965],\n",
       "         [ 0.2843, -0.1761],\n",
       "         [ 0.2813, -0.1726],\n",
       "         [ 0.2987, -0.1738],\n",
       "         [ 0.3161, -0.1763],\n",
       "         [ 0.3030, -0.1788],\n",
       "         [ 0.3212, -0.1723],\n",
       "         [ 0.3287, -0.1775],\n",
       "         [ 0.3164, -0.1719],\n",
       "         [ 0.3205, -0.1557],\n",
       "         [ 0.3101, -0.1675]]], grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "model(input_ids=ids,labels=labels,attention_mask=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}