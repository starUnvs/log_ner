{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('sid': conda)"
  },
  "interpreter": {
   "hash": "837e0a21b5e9228099062a9dae5fb2aa35e7ac1e3ac632d911fe66cd8bbd802f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('/home/dell/sid/sys_data/spark/spark_logpai.log')\n",
    "lines=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_pt=re.compile(r'([,\\s$@<>\\[\\]\\'\"=]|\\((?!\\))|(?<!\\()\\)|[:\\.](?=\\s))')\n",
    "hm_pt=re.compile(r'(^(?:\\S+\\s){4})(.*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header=[]\n",
    "msg=[]\n",
    "for line in lines:\n",
    "    if line[:3] not in set(['15/','16/','17/']):\n",
    "        continue\n",
    "    h,m=hm_pt.match(line).groups()\n",
    "    \n",
    "    header.append(h)\n",
    "    msg.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(header))\n",
    "print(header[0])\n",
    "print(msg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_tokens=[]\n",
    "for h in header:\n",
    "    tokens=[token for token in re.split(r'([\\s$@<>\\[\\]\\'\"=]|\\((?!\\))|(?<!\\()\\)|[:\\.,](?=\\s)|(?<=\\b[a-zA-Z]+):|:(?=[a-zA-Z]+\\b))',h) if token!='' and token!=None]\n",
    "    header_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_tokens=[]\n",
    "for m in msg:\n",
    "    tokens=[token for token in split_pt.split(m) if token!='' and token!=None]\n",
    "    msg_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'header_tokens':header_tokens,'msg_tokens':msg_tokens}).to_csv('tokens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_anno=[]\n",
    "for tokens in header_tokens:\n",
    "    anno=['O']*len(tokens)\n",
    "    anno[0],anno[2],anno[4],anno[6]='B-DATE','B-TIME','B-LVL','B-CLS'\n",
    "\n",
    "    header_anno.append(anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(header_tokens[0],header_anno[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_anno=[]\n",
    "for tokens in msg_tokens:\n",
    "    anno=['O']*len(tokens)\n",
    "    msg_anno.append(anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_date=r'\\d{2}\\/\\d{2}\\/\\d{4}'\n",
    "re_time=r'^\\d{2}:\\d{2}$'\n",
    "\n",
    "re_obj=r'[a-zA-Z]+-\\d{1,3}$'\n",
    "re_cls=r'(\\.([A-Z]+[a-z]*)+)|([A-Z]+[a-z]+){2,}$'\n",
    "re_func=r'\\S*\\(\\)$'\n",
    "re_path=r'^\\'?\\/[a-zA-Z]'\n",
    "re_ip=r'(?:[0-9]{1,3}\\.){3}[0-9]{1,3}'\n",
    "\n",
    "re_host=r':\\d{3,5}$'\n",
    "re_url=r'[a-z]+:\\/\\/\\S*'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_tag(ntokens,ntags, patt_tag_list):\n",
    "\n",
    "    def _sub_tag(ntokens,ntags, patt, target_tag):\n",
    "        for tokens,tags in zip(ntokens,ntags):\n",
    "            for i in range(len(tokens)):\n",
    "                if re.search(patt,tokens[i]):\n",
    "                    tags[i]=target_tag\n",
    "\n",
    "    for patt,tag in tqdm(patt_tag_list):\n",
    "        _sub_tag(ntokens,ntags,patt,tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_tag(msg_tokens,msg_anno,\n",
    "    [(re_date,'B-DATE'),\n",
    "    (re_time,'B-TIME'),\n",
    "    (re_cls,'B-CLS'),\n",
    "    (re_func,'B-FUNC'),\n",
    "    (re_path,'B-PATH'),\n",
    "    (re_ip,'B-IP'),\n",
    "    (re_host,'B-HOST'),\n",
    "    (re_url,'B-URL')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(msg_tokens[0],msg_anno[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./header_check.txt','w') as f:\n",
    "    for tokens,tags in zip(header_tokens,header_anno):\n",
    "        for token,tag in zip(tokens,tags):\n",
    "            f.write(token+' '+tag+'\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./msg_check.txt','w') as f:\n",
    "    for tokens,tags in zip(msg_tokens,msg_anno):\n",
    "        for token,tag in zip(tokens,tags):\n",
    "            f.write(token+' '+tag+'\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\n",
    "    'tokens':[h+m for h,m in zip(header_tokens,msg_tokens)],\n",
    "    'tags':[h+m for h,m in zip(header_anno,msg_anno)],\n",
    "    'raw_log':[h+m for h,m in zip(header,msg)]\n",
    "    })\n",
    "df.to_csv('./spark_ds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('spark_ds.csv','r') as f:\n",
    "    lines=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=[re.sub('\\'<O>\\'','\\'O\\'',line) for line in lines]\n",
    "with open('spark_ds_new.csv','w') as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}