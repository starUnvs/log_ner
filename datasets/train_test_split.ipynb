{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('sid': conda)"
  },
  "interpreter": {
   "hash": "837e0a21b5e9228099062a9dae5fb2aa35e7ac1e3ac632d911fe66cd8bbd802f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import glob\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "paths=glob.glob('./*/*_ds.csv')\n",
    "train_set=[]\n",
    "test_set=[]\n",
    "for fpath in paths:\n",
    "    df=pd.read_csv(fpath,index_col=0).drop_duplicates().sample(10000)\n",
    "    train_df=df.sample(8000)\n",
    "    test_df=pd.concat([train_df,df]).drop_duplicates(keep=False)\n",
    "\n",
    "    train_set.append(train_df)\n",
    "    test_set.append(test_df)\n",
    "train=pd.concat(train_set,ignore_index=True)\n",
    "test=pd.concat(test_set,ignore_index=True)\n",
    "train.to_csv('./train_full.csv')\n",
    "test.to_csv('./test_full.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "paths = ['./nongshanghang/nongshanghang_ds.csv',\n",
    "         './HDFS/hdfs_ds.csv',\n",
    "         './spark/spark_ds.csv',\n",
    "         './apache/apache_ds.csv',\n",
    "         './openstack/openstack_ds.csv',\n",
    "         './kafka/kafka_ds.csv'\n",
    "         ]\n",
    "train_set = []\n",
    "for fpath in paths:\n",
    "    df = pd.read_csv(fpath, index_col=0).drop_duplicates().sample(5000)\n",
    "    train_set.append(df)\n",
    "\n",
    "train = pd.concat(train_set, ignore_index=True)\n",
    "test = pd.read_csv('./Hadoop/hadoop_ds.csv').drop_duplicates().sample(5000)\n",
    "\n",
    "train.to_csv('./train_without_hadoop.csv')\n",
    "test.to_csv('./test_hadoop.csv')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "paths = [\n",
    "    './HDFS/hdfs_ds.csv',\n",
    "    './spark/spark_ds.csv',\n",
    "    './apache/apache_ds.csv',\n",
    "    './openstack/openstack_ds.csv',\n",
    "    './kafka/kafka_ds.csv',\n",
    "    './Hadoop/hadoop_ds.csv'\n",
    "]\n",
    "train_set = []\n",
    "for fpath in paths:\n",
    "    df = pd.read_csv(fpath, index_col=0).drop_duplicates().sample(5000)\n",
    "    train_set.append(df)\n",
    "\n",
    "train = pd.concat(train_set, ignore_index=True)\n",
    "test = pd.read_csv('./nongshanghang/nongshanghang_ds.csv').drop_duplicates().sample(5000)\n",
    "\n",
    "train.to_csv('./train_without_nongshanghang.csv')\n",
    "test.to_csv('./test_nongshanghang.csv')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "paths = [\n",
    "    './HDFS/hdfs_ds.csv',\n",
    "    './apache/apache_ds.csv',\n",
    "    './openstack/openstack_ds.csv',\n",
    "    './kafka/kafka_ds.csv',\n",
    "    './Hadoop/hadoop_ds.csv',\n",
    "    './nongshanghang/nongshanghang_ds.csv',\n",
    "]\n",
    "train_set = []\n",
    "for fpath in paths:\n",
    "    df = pd.read_csv(fpath, index_col=0).drop_duplicates().sample(5000)\n",
    "    train_set.append(df)\n",
    "\n",
    "train = pd.concat(train_set, ignore_index=True)\n",
    "test = pd.read_csv('./spark/spark_ds.csv').drop_duplicates().sample(5000)\n",
    "\n",
    "train.to_csv('./train_without_spark.csv')\n",
    "test.to_csv('./test_spark.csv')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}